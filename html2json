#!/usr/bin/env python3

import argparse
import json
import re
import requests
from bs4 import BeautifulSoup
from oglaf import TomeOfKnowledge
from time import sleep


archive_json    = 'archive.json'


parser = argparse.ArgumentParser(description='Oglaf comic metadata scraper')
parser.add_argument('-f', metavar='filename', action='store',
    help='Filename - Provide a custom JSON output filename instead of the default "' + archive_json + '"')
parser.add_argument('-r', action='store_true',
    help='Rebuild - Build new JSON from website')
parser.add_argument('-u', action='store_true',
    help='Update - Read existing JSON and append new entries only [DEFAULT]')
args = parser.parse_args()

if args.f:
    archive_json = args.f


comics = dict()
newcomics = dict()
session = requests.Session()


def get_strip_urls(soup):
    ''' Given the file system location of a strip, find child strip locations '''
    # Get the canonical URL for the current page then delete it
    canonical_link = soup.find('link', attrs={'rel':'canonical'})
    cur_page = canonical_link.get('href')
    canonical_link.decompose()

    # Delete the "prev" link so we don't process it later
    prev_link = soup.find('a', attrs={'rel':'prev'})
    if prev_link is not None:
        prev_link.decompose()

    # Get the "next" link then delete it
    next_link = soup.find('a', attrs={'rel':'next'})
    next_page = None
    if next_link is not None:
        next_page = next_link.get('href')
        next_link.decompose()

    # Convert current URI for comparison: https://www.oglaf.com/glove/2/ -> /glove/
    cur_uri = re.sub(r'^' + TomeOfKnowledge.PROTOHOST, '', cur_page, flags=re.IGNORECASE)
    cur_uri = re.sub(r'/\d+/$', '/', cur_uri)
    cur_uri = cur_uri.rstrip('/')

    # Find occasional epilogue or other add-ons (like /roughtrade/vocab/)
    epi_page = None
    for url in soup.find_all('a'):
        href = url.get('href')
        if cur_uri in href:
            epi_page = href

    urls = [ cur_page ]

    next_page_text = None
    if epi_page is not None:
        next_page_text = get_page(TomeOfKnowledge.PROTOHOST + epi_page)
    if next_page is not None and cur_uri in next_page:
        next_page_text = get_page(TomeOfKnowledge.PROTOHOST + next_page)

    if next_page_text is not None:
        npsoup = BeautifulSoup(next_page_text, 'html.parser')
        urls = urls + get_strip_urls(npsoup)

    return urls


def get_page(url):
    ''' Get a URL and return the content text, or None on error '''
    #print(url)
    r = session.get(url, cookies={'AGE_CONFIRMED':'yes'})
    if r.status_code == requests.codes.ok:
        return r.text
    elif r.status_code == 421:
        sleep(5)
        return get_page(url)
    else:
        print("Oops getting page {} ({})".format(url, r.status_code))
        return None


# Slurp in the archive to parse comic strips
archive = get_page(TomeOfKnowledge.ARCHIVE_URL)
soup = BeautifulSoup(archive, 'html.parser')


# Load existing JSON if we're UPDATING
if args.u or args.r is False:
    with open(archive_json, 'r') as curfile:
        curdata = curfile.read()
    comics = json.loads(curdata)


# Evaluate each strip in the archive
# Navigation relies on style elements in the archive page
#
# There's at least one strip that can be found by navigating "next" / 
# "previous" that's not in the archive. (I'm looking at you 
# https://www.oglaf.com/trueslut2012/)
#
# Instead of walking the archive links, start at the first entry in the
# archive then walk backwards using the "previous" links on each strip
#
strip_number = 0

###
strip = soup.find('img', attrs={'width': '400', 'height': '100'}).find_parent('a').get('href')
if strip is None:
    print("Couldn't find a single strip in the archive!?")
    exit()

while strip is not None:
    strip = get_page(TomeOfKnowledge.PROTOHOST + strip)
    s = BeautifulSoup(strip, 'html.parser')
    strip_title = re.sub(r' page 1', '', s.title.string, flags=re.IGNORECASE)
    prev_strip = s.find('a', attrs={'rel':'prev'})
    if prev_strip is not None:
        prev_strip = prev_strip.get('href')

    if strip_title not in comics.keys(): 
        # Enumerate all page links for this one, including epilogues
        strip_urls = get_strip_urls(s)

        # Populate ordered list (newest first)
        newcomics[strip_number] = dict()
        newcomics[strip_number]['title'] = strip_title
        newcomics[strip_number]['urls'] = strip_urls
        strip_number += 1
        strip = prev_strip
    else:
        print('Adding ' + str(len(newcomics)) + ' new entries to ' + archive_json)
        # Let's ASSume we're only adding new entries, so when we find the first hit in
        # existing metadata, I guess we're up to date
        break


###
#for strip in soup.find_all('img', attrs={'width': '400', 'height': '100'}):
#
#    # Get title for this one
#    parent_href = strip.find_parent('a').get('href')
#    strip = get_page(TomeOfKnowledge.PROTOHOST + parent_href)
#    s = BeautifulSoup(strip, 'html.parser')
#    strip_title = re.sub(r' page 1', '', s.title.string, flags=re.IGNORECASE)
#
#    if strip_title not in comics.keys(): 
#        # Enumerate all page links for this one, including epilogues
#        strip_urls = get_strip_urls(s)
#
#        # Populate ordered list (newest first)
#        newcomics[strip_number] = dict()
#        newcomics[strip_number]['title'] = strip_title
#        newcomics[strip_number]['urls'] = strip_urls
#        strip_number += 1
#    else:
#        print('Adding ' + str(len(newcomics)) + ' new entries to ' + archive_json)
#        # Let's ASSume we're only adding new entries, so when we find the first hit in
#        # existing metadata, I guess we're up to date
#        break


# Add new comics discovered onto the stack of known comics (if updating), in
# publish order.
num = len(comics)
for incr in sorted(newcomics.keys(), reverse=True):
    title = newcomics[incr]['title']
    comics[title] = dict()
    comics[title]['urls'] = newcomics[incr]['urls']
    comics[title]['publishOrder'] = num
    num += 1


# Print JSON to file
with open(archive_json, 'w') as outfile:
    json.dump(comics, outfile, indent=2)

